## A Heat environment file which can be used to set up storage
## backends. Defaults to Ceph used as a backend for Cinder, Glance and
## Nova ephemeral storage.
resource_registry:
  OS::TripleO::NodeUserData: /home/stack/wipe-disks.yaml
  OS::TripleO::Services::CephMon: /usr/share/openstack-tripleo-heat-templates/puppet/services/ceph-mon.yaml
  OS::TripleO::Services::CephOSD: /usr/share/openstack-tripleo-heat-templates/puppet/services/ceph-osd.yaml
  OS::TripleO::Services::CephClient: /usr/share/openstack-tripleo-heat-templates/puppet/services/ceph-client.yaml

parameter_defaults:

  #### BACKEND SELECTION ####

  ## Whether to enable iscsi backend for Cinder.
  CinderEnableIscsiBackend: false
  ## Whether to enable rbd (Ceph) backend for Cinder.
  CinderEnableRbdBackend: true
  ## Whether to enable NFS backend for Cinder.
  # CinderEnableNfsBackend: false
  ## Whether to enable rbd (Ceph) backend for Nova ephemeral storage.
  NovaEnableRbdBackend: true
  ## Glance backend can be either 'rbd' (Ceph), 'swift' or 'file'.
  GlanceBackend: rbd
  ## Gnocchi backend can be either 'rbd' (Ceph), 'swift' or 'file'.
  GnocchiBackend: rbd


  #### CINDER NFS SETTINGS ####

  ## NFS mount options
  # CinderNfsMountOptions: ''
  ## NFS mount point, e.g. '192.168.122.1:/export/cinder'
  # CinderNfsServers: ''


  #### GLANCE FILE BACKEND PACEMAKER SETTINGS (used for mounting NFS) ####

  ## Whether to make Glance 'file' backend a mount managed by Pacemaker
  # GlanceFilePcmkManage: false
  ## File system type of the mount
  # GlanceFilePcmkFstype: nfs
  ## Pacemaker mount point, e.g. '192.168.122.1:/export/glance' for NFS
  ##  (If using IPv6, use both double- and single-quotes,
  ##  e.g. "'[fdd0::1]:/export/glance'")
  # GlanceFilePcmkDevice: ''
  ## Options for the mount managed by Pacemaker
  # GlanceFilePcmkOptions: ''


  #### CEPH SETTINGS ####

  ## When deploying Ceph Nodes through the oscplugin CLI, the following
  ## parameters are set automatically by the CLI. When deploying via
  ## heat stack-create or ceph on the controller nodes only,
  ## they need to be provided manually.
  #
  ExtraConfig:
    ceph::profile::params::osd_pool_default_pg_num: 4000
    ceph::conf::args:
      global/max_open_files:
        value: 131072
    ceph::profile::params::osds:
      '/dev/sdb':
        journal: '/dev/nvme0n1'
      '/dev/sdc':
        journal: '/dev/nvme0n1'
      '/dev/sdd':
        journal: '/dev/nvme0n1'
      '/dev/sde':
        journal: '/dev/nvme0n1'
      '/dev/sdf':
        journal: '/dev/nvme0n1'
      '/dev/sdg':
        journal: '/dev/nvme0n1'
      '/dev/sdh':
        journal: '/dev/nvme0n1'
      '/dev/sdi':
        journal: '/dev/nvme0n1'
      '/dev/sdj':
        journal: '/dev/nvme0n1'
      '/dev/sdk':
        journal: '/dev/nvme0n1'
      '/dev/sdl':
        journal: '/dev/nvme0n1'
      '/dev/sdm':
        journal: '/dev/nvme0n1'
      '/dev/sdn':
        journal: '/dev/nvme0n1'
      '/dev/sdo':
        journal: '/dev/nvme0n1'
      '/dev/sdp':
        journal: '/dev/nvme0n1'
      '/dev/sdq':
        journal: '/dev/nvme0n1'

  ## Number of Ceph storage nodes to deploy
  #CephStorageCount: 0
  ## Ceph FSID, e.g. '4b5c8c0a-ff60-454b-a1b4-9747aa737d19'
  # CephClusterFSID: ''
  ## Ceph monitor key, e.g. 'AQC+Ox1VmEr3BxAALZejqeHj50Nj6wJDvs96OQ=='
  # CephMonKey: ''
  ## Ceph admin key, e.g. 'AQDLOh1VgEp6FRAAFzT7Zw+Y9V6JJExQAsRnRQ=='
  # CephAdminKey: ''
  ## Ceph client key, e.g 'AQC+vYNXgDAgAhAAc8UoYt+OTz5uhV7ItLdwUw=='
  # CephClientKey: ''
